{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE+qM/ILQ3dOMIB2CfRWUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Communist02/BIN-22-1_Mazur_Denis_ML/blob/master/DZ4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATt81nfMRy9M",
        "outputId": "c9453adf-ec4c-415e-c209-06014cbe7782"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E-NxB4xSXL3",
        "outputId": "e4d1684d-dfd8-40d2-c0e0-51a7e2a5cc22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import math\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "class Processing:\n",
        "    def tokenize(self, text: str) -> list[str]:\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def lematize(self, tokens: list[str]) -> list[str]:\n",
        "        morph3 = MorphAnalyzer()\n",
        "        lemmatized_words = [morph3.parse(\n",
        "            word)[0].normal_form for word in tokens]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def stemming(self, tokens: list[str]) -> list[str]:\n",
        "        stemmer = SnowballStemmer(\"russian\")\n",
        "        lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def vectorize(self, tokens: list[str]) -> list[int]:\n",
        "        dict_vectors = {}\n",
        "        result = []\n",
        "        for word in tokens:\n",
        "            if word in dict_vectors.keys():\n",
        "                result.append(dict_vectors[word])\n",
        "            else:\n",
        "                dict_vectors[word] = len(dict_vectors)\n",
        "                result.append(dict_vectors[word])\n",
        "        return result\n",
        "\n",
        "    def vectorize_dict(self, tokens: list[str]) -> list[int]:\n",
        "        dict_vectors = {}\n",
        "        result = []\n",
        "        for word in tokens:\n",
        "            if word not in dict_vectors.keys():\n",
        "                dict_vectors[word] = len(dict_vectors)\n",
        "        return dict_vectors\n",
        "\n",
        "    def delete_stop_words(self, tokens: list[str]) -> list[int]:\n",
        "        stop_words = set(stopwords.words('russian')).union(['.', ',', ':', '?', '!'])\n",
        "        return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    def bag_of_words(self, tokens: list[str]) -> dict[str]:\n",
        "        dict_words = {}\n",
        "        for word in tokens:\n",
        "            dict_words[word] = dict_words.setdefault(word, 0) + 1\n",
        "        return dict_words\n",
        "\n",
        "    def tf(self, tokens: list[str]) -> dict[str]:\n",
        "        dict_words = self.bag_of_words(tokens)\n",
        "        for word in dict_words:\n",
        "            dict_words[word] /= len(tokens)\n",
        "        return dict_words\n",
        "\n",
        "    def idf(self, texts: list[list[str]]) -> dict[str]:\n",
        "        dict_words = {}\n",
        "        big_text = []\n",
        "        for text in texts:\n",
        "            big_text += list(set(text))\n",
        "        for word in set(big_text):\n",
        "            dict_words[word] = math.log(len(texts) / big_text.count(word))\n",
        "        return dict_words\n",
        "\n",
        "    def tf_idf(self, texts: list[list[str]], indexText: int) -> dict[str]:\n",
        "        tf = self.tf(texts[indexText])\n",
        "        idf = self.idf(texts)\n",
        "        dict_words = {}\n",
        "        for word in tf:\n",
        "            dict_words[word] = tf[word] * idf[word]\n",
        "        return dict_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk_mRwppWFHO",
        "outputId": "e36e8d95-43e0-4963-8abc-264151d7d643"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer:\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, lr=0.01):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.lr = lr\n",
        "\n",
        "        # Параметры модели\n",
        "        self.embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
        "\n",
        "        self.W_q = np.random.randn(embed_dim, hidden_dim) * 0.01\n",
        "        self.W_k = np.random.randn(embed_dim, hidden_dim) * 0.01\n",
        "        self.W_v = np.random.randn(embed_dim, hidden_dim) * 0.01\n",
        "\n",
        "        self.W_out = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp = np.exp(x - np.max(x))\n",
        "        return exp / np.sum(exp)\n",
        "\n",
        "    def forward(self, input_indices):\n",
        "        self.input_indices = input_indices\n",
        "        self.x = self.embeddings[input_indices]  # shape: (seq_len, embed_dim)\n",
        "\n",
        "        self.Q = self.x @ self.W_q  # (seq_len, hidden_dim)\n",
        "        self.K = self.x @ self.W_k\n",
        "        self.V = self.x @ self.W_v\n",
        "\n",
        "        self.attn_scores = self.Q @ self.K.T / np.sqrt(self.embed_dim)  # (seq_len, seq_len)\n",
        "        self.attn_weights = self.softmax(self.attn_scores)  # (seq_len, seq_len)\n",
        "\n",
        "        self.attended = self.attn_weights @ self.V  # (seq_len, hidden_dim)\n",
        "        self.context = np.mean(self.attended, axis=0)  # (hidden_dim,)\n",
        "\n",
        "        self.logits = self.context @ self.W_out  # (vocab_size,)\n",
        "        self.probs = self.softmax(self.logits)\n",
        "\n",
        "        return self.probs\n",
        "\n",
        "    def backward(self, target_index):\n",
        "        # dL/dlogits = probs - y_one_hot\n",
        "        dlogits = self.probs.copy()\n",
        "        dlogits[target_index] -= 1  # (vocab_size,)\n",
        "\n",
        "        # dL/dW_out\n",
        "        dW_out = np.outer(self.context, dlogits)  # (hidden_dim, vocab_size)\n",
        "\n",
        "        # dL/dcontext\n",
        "        dcontext = dlogits @ self.W_out.T  # (hidden_dim,)\n",
        "\n",
        "        # dcontext -> dattended (т.к. mean)\n",
        "        dattended = np.ones_like(self.attended) * dcontext / self.attended.shape[0]  # (seq_len, hidden_dim)\n",
        "\n",
        "        dV = self.attn_weights.T @ dattended  # (seq_len, hidden_dim)\n",
        "        d_attn_weights = dattended @ self.V.T  # (seq_len, seq_len)\n",
        "\n",
        "        d_scores = d_attn_weights * self.attn_weights * (1 - self.attn_weights)  # (seq_len, seq_len)\n",
        "\n",
        "        dQ = d_scores @ self.K / np.sqrt(self.embed_dim)\n",
        "        dK = d_scores.T @ self.Q / np.sqrt(self.embed_dim)\n",
        "\n",
        "        # dW_q, dW_k, dW_v\n",
        "        dW_q = self.x.T @ dQ\n",
        "        dW_k = self.x.T @ dK\n",
        "        dW_v = self.x.T @ dV\n",
        "\n",
        "        # dEmbeddings\n",
        "        dx_q = dQ @ self.W_q.T\n",
        "        dx_k = dK @ self.W_k.T\n",
        "        dx_v = dV @ self.W_v.T\n",
        "        dx = dx_q + dx_k + dx_v  # (seq_len, embed_dim)\n",
        "\n",
        "        # Градиенты по эмбеддингам\n",
        "        for i, idx in enumerate(self.input_indices):\n",
        "            self.embeddings[idx] -= self.lr * dx[i]\n",
        "\n",
        "        # Обновление параметров\n",
        "        self.W_out -= self.lr * dW_out\n",
        "        self.W_q -= self.lr * dW_q\n",
        "        self.W_k -= self.lr * dW_k\n",
        "        self.W_v -= self.lr * dW_v\n",
        "\n",
        "    def train_step(self, input_indices, target_index):\n",
        "        self.forward(input_indices)\n",
        "        self.backward(target_index)\n",
        "\n",
        "    def predict(self, input_indices):\n",
        "        probs = self.forward(input_indices)\n",
        "        return np.argmax(probs)"
      ],
      "metadata": {
        "id": "LaiTsvTwSMtC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "pc = Processing()\n",
        "text = 'Звезды отражались в её глазах. Раньше, еще месяц назад их не было видно в черте города. Свет фонарей и смог заглушали их слабое мерцание. Все изменилось. И одним из немногих плюсов сложившейся ситуации было мерцание звезд в её глазах, и воздух, кажется стал чище. У всех у нас когда то была работа, и был дом. У некоторых были дети. У Лены была дочка. Она работала барменшой, а по вечерам подрабатывала в \"клубе знакомств\". Попросту говоря - была проституткой. Теперь ей уже не приходится ездить по незнакомым клиентам, каждый раз перед дверью квартиры креститься, и молиться, что бы все прошло как надо. Это тоже плюс. Но теперь у неё нет дочки. Она потерялась в первые дни, как только все это начиналось. Лена была \"на вызове\", когда исчезло электричество. Никто еще не знал, что это серьезно. Мобильная связь не работала, город погрузился во тьму за окнами однакомнатной квартиры, в которой возбужденный мужчина кончал в презерватив, а Лена считала секунды до очередного вызова. Она не могла как обычно принять душ, и вызвать такси, и после осознания этого, просто начала одеваться. Белье по привычке было сложено одной кучкой рядом с кроватью. Мужчина, имя которого она не захотела запоминать сказал ей спасибо и открыл дверь, что то проворчав напоследок на \"долбанных электриков\"... Лене было очень приятно выйти на свежий воздух, после пропахшей перегаром комнатушки. Она шла по темным улицам города, шла на \"базу\" пешком, и эта непроглядная тьма вокруг для неё сейчас была отражением внутреннего состояния, и поэтому она наслаждалась этой прогулкой. Она еще не знала, что электричество и водоснабжение уже не восстановят. Она не могла подумать, что через три часа её пятилетняя дочка, испугавшись темноты и одиночества, выйдет из квартиры, и пропадет навсегда. Она еще не знала, что её поиски будут бесполезны и опасны... Она просто шла по улице.'\n",
        "\n",
        "orders = [\n",
        "    'Звезды отражались',\n",
        "    'Воздух стал',\n",
        "    'Она раньше шла',\n",
        "    'Дочка испугалась',\n",
        "    'Город погрузился',\n",
        "    'Она не могла месяц',\n",
        "    'Электричество исчезло',\n",
        "    'Поиск видно'\n",
        "]\n",
        "\n",
        "result = [\n",
        "    'глазах',\n",
        "    'чище',\n",
        "    'темным',\n",
        "    'одиночества',\n",
        "    'тьму',\n",
        "    'связь',\n",
        "    'связь',\n",
        "    'опасен'\n",
        "]\n",
        "\n",
        "tokens = pc.delete_stop_words(pc.lematize(pc.tokenize(text)))\n",
        "\n",
        "# Векторизация словаря\n",
        "vocab = list(set(tokens))\n",
        "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
        "index_to_word = {i: w for w, i in word_to_index.items()}\n",
        "\n",
        "# Пример данных\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Задача - предсказать последнее слово\n",
        "for phrase, label in zip(orders, result):\n",
        "    tokens = pc.delete_stop_words(pc.lematize(pc.tokenize(phrase)))\n",
        "    if len(tokens) < 2: continue\n",
        "    input_indices = [word_to_index.get(w, 0) for w in tokens if w in word_to_index]\n",
        "    label_index = word_to_index.get(label, 0)\n",
        "    X.append(input_indices)\n",
        "    y.append(label_index)\n",
        "\n",
        "# Обучение\n",
        "model = SimpleTransformer(vocab_size=len(vocab), embed_dim=16, hidden_dim=32)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    total_loss = 0\n",
        "    for i in range(len(X)):\n",
        "        model.train_step(X[i], y[i])\n",
        "        probs = model.forward(X[i])\n",
        "        total_loss += -np.log(probs[y[i]] + 1e-9)\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(X)}\")\n",
        "\n",
        "# Предсказания\n",
        "print(\"Predictions:\")\n",
        "for phrase in orders:\n",
        "    tokens = pc.delete_stop_words(pc.lematize(pc.tokenize(phrase)))\n",
        "    input_indices = [word_to_index.get(w, 0) for w in tokens if w in word_to_index]\n",
        "    pred_index = model.predict(input_indices)\n",
        "    print(f\"{phrase} -> {index_to_word[pred_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl-kXzZ0TFf-",
        "outputId": "93befe2e-837c-4379-9dde-ecd7cdf5e128"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.948759121512369\n",
            "Predictions:\n",
            "Звезды отражались -> потеряться\n",
            "Воздух стал -> комнатушка\n",
            "Она раньше шла -> улица\n",
            "Дочка испугалась -> очень\n",
            "Город погрузился -> потеряться\n",
            "Она не могла месяц -> связь\n",
            "Электричество исчезло -> связь\n",
            "Поиск видно -> потеряться\n"
          ]
        }
      ]
    }
  ]
}