{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZBLWu0y65mu5f3aqWN1rX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Communist02/BIN-22-1_Mazur_Denis_ML/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATt81nfMRy9M",
        "outputId": "351ee193-8127-47c8-ea5d-09fb9a0863d1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E-NxB4xSXL3",
        "outputId": "7382ff32-969e-4d83-f7db-670fecd966b3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import math\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "class Processing:\n",
        "    def tokenize(self, text: str) -> list[str]:\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def lematize(self, tokens: list[str]) -> list[str]:\n",
        "        morph3 = MorphAnalyzer()\n",
        "        lemmatized_words = [morph3.parse(\n",
        "            word)[0].normal_form for word in tokens]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def stemming(self, tokens: list[str]) -> list[str]:\n",
        "        stemmer = SnowballStemmer(\"russian\")\n",
        "        lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def vectorize(self, tokens: list[str]) -> list[int]:\n",
        "        dict_vectors = {}\n",
        "        result = []\n",
        "        for word in tokens:\n",
        "            if word in dict_vectors.keys():\n",
        "                result.append(dict_vectors[word])\n",
        "            else:\n",
        "                dict_vectors[word] = len(dict_vectors)\n",
        "                result.append(dict_vectors[word])\n",
        "        return result\n",
        "\n",
        "    def vectorize_dict(self, tokens: list[str]) -> list[int]:\n",
        "        dict_vectors = {}\n",
        "        result = []\n",
        "        for word in tokens:\n",
        "            if word not in dict_vectors.keys():\n",
        "                dict_vectors[word] = len(dict_vectors)\n",
        "        return dict_vectors\n",
        "\n",
        "    def delete_stop_words(self, tokens: list[str]) -> list[int]:\n",
        "        stop_words = set(stopwords.words('russian')).union(['.', ',', ':', '?', '!'])\n",
        "        return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    def bag_of_words(self, tokens: list[str]) -> dict[str]:\n",
        "        dict_words = {}\n",
        "        for word in tokens:\n",
        "            dict_words[word] = dict_words.setdefault(word, 0) + 1\n",
        "        return dict_words\n",
        "\n",
        "    def tf(self, tokens: list[str]) -> dict[str]:\n",
        "        dict_words = self.bag_of_words(tokens)\n",
        "        for word in dict_words:\n",
        "            dict_words[word] /= len(tokens)\n",
        "        return dict_words\n",
        "\n",
        "    def idf(self, texts: list[list[str]]) -> dict[str]:\n",
        "        dict_words = {}\n",
        "        big_text = []\n",
        "        for text in texts:\n",
        "            big_text += list(set(text))\n",
        "        for word in set(big_text):\n",
        "            dict_words[word] = math.log(len(texts) / big_text.count(word))\n",
        "        return dict_words\n",
        "\n",
        "    def tf_idf(self, texts: list[list[str]], indexText: int) -> dict[str]:\n",
        "        tf = self.tf(texts[indexText])\n",
        "        idf = self.idf(texts)\n",
        "        dict_words = {}\n",
        "        for word in tf:\n",
        "            dict_words[word] = tf[word] * idf[word]\n",
        "        return dict_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk_mRwppWFHO",
        "outputId": "731e4079-2403-45ed-e1b6-159211f48f43"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleTransformer:\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Случайные эмбеддинги для токенов\n",
        "        self.embeddings = np.random.randn(vocab_size, embed_dim)\n",
        "\n",
        "        # Один \"слой внимания\" (упрощённо)\n",
        "        self.W_q = np.random.randn(embed_dim, hidden_dim)\n",
        "        self.W_k = np.random.randn(embed_dim, hidden_dim)\n",
        "        self.W_v = np.random.randn(embed_dim, hidden_dim)\n",
        "\n",
        "        # Выходной слой\n",
        "        self.W_out = np.random.randn(hidden_dim, vocab_size)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp / np.sum(exp, axis=-1, keepdims=True)\n",
        "\n",
        "    def forward(self, input_indices):\n",
        "        # Получаем эмбеддинги\n",
        "        x = self.embeddings[input_indices]  # shape: (seq_len, embed_dim)\n",
        "\n",
        "        # Внимание (один шаг, без маски)\n",
        "        Q = x @ self.W_q\n",
        "        K = x @ self.W_k\n",
        "        V = x @ self.W_v\n",
        "\n",
        "        attn_scores = Q @ K.T / np.sqrt(self.embed_dim)\n",
        "        attn_weights = self.softmax(attn_scores)\n",
        "\n",
        "        attended = attn_weights @ V  # shape: (seq_len, hidden_dim)\n",
        "\n",
        "        # Среднее по токенам (по сути — пулинг)\n",
        "        context_vector = np.mean(attended, axis=0)\n",
        "\n",
        "        # Финальный предикт\n",
        "        logits = context_vector @ self.W_out  # shape: (vocab_size,)\n",
        "        probs = self.softmax(logits)\n",
        "        return probs\n",
        "\n",
        "    def predict(self, input_indices):\n",
        "        probs = self.forward(input_indices)\n",
        "        return np.argmax(probs)\n"
      ],
      "metadata": {
        "id": "LaiTsvTwSMtC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc = Processing()\n",
        "text = 'Звезды отражались в её глазах. Раньше, еще месяц назад их не было видно в черте города. Свет фонарей и смог заглушали их слабое мерцание. Все изменилось. И одним из немногих плюсов сложившейся ситуации было мерцание звезд в её глазах, и воздух, кажется стал чище. У всех у нас когда то была работа, и был дом. У некоторых были дети. У Лены была дочка. Она работала барменшой, а по вечерам подрабатывала в \"клубе знакомств\". Попросту говоря - была проституткой. Теперь ей уже не приходится ездить по незнакомым клиентам, каждый раз перед дверью квартиры креститься, и молиться, что бы все прошло как надо. Это тоже плюс. Но теперь у неё нет дочки. Она потерялась в первые дни, как только все это начиналось. Лена была \"на вызове\", когда исчезло электричество. Никто еще не знал, что это серьезно. Мобильная связь не работала, город погрузился во тьму за окнами однакомнатной квартиры, в которой возбужденный мужчина кончал в презерватив, а Лена считала секунды до очередного вызова. Она не могла как обычно принять душ, и вызвать такси, и после осознания этого, просто начала одеваться. Белье по привычке было сложено одной кучкой рядом с кроватью. Мужчина, имя которого она не захотела запоминать сказал ей спасибо и открыл дверь, что то проворчав напоследок на \"долбанных электриков\"... Лене было очень приятно выйти на свежий воздух, после пропахшей перегаром комнатушки. Она шла по темным улицам города, шла на \"базу\" пешком, и эта непроглядная тьма вокруг для неё сейчас была отражением внутреннего состояния, и поэтому она наслаждалась этой прогулкой. Она еще не знала, что электричество и водоснабжение уже не восстановят. Она не могла подумать, что через три часа её пятилетняя дочка, испугавшись темноты и одиночества, выйдет из квартиры, и пропадет навсегда. Она еще не знала, что её поиски будут бесполезны и опасны... Она просто шла по улице.'\n",
        "\n",
        "orders = [\n",
        "    'Звезды отражались',\n",
        "    'Воздух стал',\n",
        "    'Она раньше шла',\n",
        "    'Дочка испугалась',\n",
        "    'Город погрузился',\n",
        "    'Она не могла месяц',\n",
        "    'Электричество исчезло',\n",
        "    'Поиск видно'\n",
        "]\n",
        "\n",
        "result = [\n",
        "    'глазах',\n",
        "    'чище',\n",
        "    'темным',\n",
        "    'одиночества',\n",
        "    'тьму',\n",
        "    'связь',\n",
        "    'связь',\n",
        "    'опасен'\n",
        "]\n",
        "\n",
        "tokens = pc.delete_stop_words(pc.lematize(pc.tokenize(text)))\n",
        "\n",
        "# Векторизация словаря\n",
        "vocab = list(set(tokens))\n",
        "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
        "index_to_word = {i: w for w, i in word_to_index.items()}\n",
        "\n",
        "# Пример данных\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Задача - предсказать последнее слово\n",
        "for phrase, label in zip(orders, result):\n",
        "    tokens = pc.delete_stop_words(pc.lematize(pc.tokenize(phrase)))\n",
        "    if len(tokens) < 2: continue\n",
        "    input_indices = [word_to_index.get(w, 0) for w in tokens if w in word_to_index]\n",
        "    label_index = word_to_index.get(label, 0)\n",
        "    X.append(input_indices)\n",
        "    y.append(label_index)\n",
        "\n",
        "# Обучение\n",
        "model = SimpleTransformer(vocab_size=len(vocab), embed_dim=16, hidden_dim=32)\n",
        "\n",
        "for epoch in range(10000):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        probs = model.forward(X[i])\n",
        "        target = y[i]\n",
        "        loss += -np.log(probs[target] + 1e-9)  # Кросс-энтропия\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(X)}\")\n",
        "\n",
        "# Предсказания\n",
        "print(\"Predictions:\")\n",
        "for phrase in orders:\n",
        "    tokens = pc.delete_stop_words(pc.lematize(pc.tokenize(phrase)))\n",
        "    input_indices = [word_to_index.get(w, 0) for w in tokens if w in word_to_index]\n",
        "    pred_index = model.predict(input_indices)\n",
        "    print(f\"{phrase} -> {index_to_word[pred_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl-kXzZ0TFf-",
        "outputId": "01873a54-fc1f-4afb-ed2b-0e3dee8a9383"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 19.26107186649898\n",
            "Epoch 100, Loss: 19.26107186649898\n",
            "Epoch 200, Loss: 19.26107186649898\n",
            "Epoch 300, Loss: 19.26107186649898\n",
            "Epoch 400, Loss: 19.26107186649898\n",
            "Epoch 500, Loss: 19.26107186649898\n",
            "Epoch 600, Loss: 19.26107186649898\n",
            "Epoch 700, Loss: 19.26107186649898\n",
            "Epoch 800, Loss: 19.26107186649898\n",
            "Epoch 900, Loss: 19.26107186649898\n",
            "Epoch 1000, Loss: 19.26107186649898\n",
            "Epoch 1100, Loss: 19.26107186649898\n",
            "Epoch 1200, Loss: 19.26107186649898\n",
            "Epoch 1300, Loss: 19.26107186649898\n",
            "Epoch 1400, Loss: 19.26107186649898\n",
            "Epoch 1500, Loss: 19.26107186649898\n",
            "Epoch 1600, Loss: 19.26107186649898\n",
            "Epoch 1700, Loss: 19.26107186649898\n",
            "Epoch 1800, Loss: 19.26107186649898\n",
            "Epoch 1900, Loss: 19.26107186649898\n",
            "Epoch 2000, Loss: 19.26107186649898\n",
            "Epoch 2100, Loss: 19.26107186649898\n",
            "Epoch 2200, Loss: 19.26107186649898\n",
            "Epoch 2300, Loss: 19.26107186649898\n",
            "Epoch 2400, Loss: 19.26107186649898\n",
            "Epoch 2500, Loss: 19.26107186649898\n",
            "Epoch 2600, Loss: 19.26107186649898\n",
            "Epoch 2700, Loss: 19.26107186649898\n",
            "Epoch 2800, Loss: 19.26107186649898\n",
            "Epoch 2900, Loss: 19.26107186649898\n",
            "Epoch 3000, Loss: 19.26107186649898\n",
            "Epoch 3100, Loss: 19.26107186649898\n",
            "Epoch 3200, Loss: 19.26107186649898\n",
            "Epoch 3300, Loss: 19.26107186649898\n",
            "Epoch 3400, Loss: 19.26107186649898\n",
            "Epoch 3500, Loss: 19.26107186649898\n",
            "Epoch 3600, Loss: 19.26107186649898\n",
            "Epoch 3700, Loss: 19.26107186649898\n",
            "Epoch 3800, Loss: 19.26107186649898\n",
            "Epoch 3900, Loss: 19.26107186649898\n",
            "Epoch 4000, Loss: 19.26107186649898\n",
            "Epoch 4100, Loss: 19.26107186649898\n",
            "Epoch 4200, Loss: 19.26107186649898\n",
            "Epoch 4300, Loss: 19.26107186649898\n",
            "Epoch 4400, Loss: 19.26107186649898\n",
            "Epoch 4500, Loss: 19.26107186649898\n",
            "Epoch 4600, Loss: 19.26107186649898\n",
            "Epoch 4700, Loss: 19.26107186649898\n",
            "Epoch 4800, Loss: 19.26107186649898\n",
            "Epoch 4900, Loss: 19.26107186649898\n",
            "Epoch 5000, Loss: 19.26107186649898\n",
            "Epoch 5100, Loss: 19.26107186649898\n",
            "Epoch 5200, Loss: 19.26107186649898\n",
            "Epoch 5300, Loss: 19.26107186649898\n",
            "Epoch 5400, Loss: 19.26107186649898\n",
            "Epoch 5500, Loss: 19.26107186649898\n",
            "Epoch 5600, Loss: 19.26107186649898\n",
            "Epoch 5700, Loss: 19.26107186649898\n",
            "Epoch 5800, Loss: 19.26107186649898\n",
            "Epoch 5900, Loss: 19.26107186649898\n",
            "Epoch 6000, Loss: 19.26107186649898\n",
            "Epoch 6100, Loss: 19.26107186649898\n",
            "Epoch 6200, Loss: 19.26107186649898\n",
            "Epoch 6300, Loss: 19.26107186649898\n",
            "Epoch 6400, Loss: 19.26107186649898\n",
            "Epoch 6500, Loss: 19.26107186649898\n",
            "Epoch 6600, Loss: 19.26107186649898\n",
            "Epoch 6700, Loss: 19.26107186649898\n",
            "Epoch 6800, Loss: 19.26107186649898\n",
            "Epoch 6900, Loss: 19.26107186649898\n",
            "Epoch 7000, Loss: 19.26107186649898\n",
            "Epoch 7100, Loss: 19.26107186649898\n",
            "Epoch 7200, Loss: 19.26107186649898\n",
            "Epoch 7300, Loss: 19.26107186649898\n",
            "Epoch 7400, Loss: 19.26107186649898\n",
            "Epoch 7500, Loss: 19.26107186649898\n",
            "Epoch 7600, Loss: 19.26107186649898\n",
            "Epoch 7700, Loss: 19.26107186649898\n",
            "Epoch 7800, Loss: 19.26107186649898\n",
            "Epoch 7900, Loss: 19.26107186649898\n",
            "Epoch 8000, Loss: 19.26107186649898\n",
            "Epoch 8100, Loss: 19.26107186649898\n",
            "Epoch 8200, Loss: 19.26107186649898\n",
            "Epoch 8300, Loss: 19.26107186649898\n",
            "Epoch 8400, Loss: 19.26107186649898\n",
            "Epoch 8500, Loss: 19.26107186649898\n",
            "Epoch 8600, Loss: 19.26107186649898\n",
            "Epoch 8700, Loss: 19.26107186649898\n",
            "Epoch 8800, Loss: 19.26107186649898\n",
            "Epoch 8900, Loss: 19.26107186649898\n",
            "Epoch 9000, Loss: 19.26107186649898\n",
            "Epoch 9100, Loss: 19.26107186649898\n",
            "Epoch 9200, Loss: 19.26107186649898\n",
            "Epoch 9300, Loss: 19.26107186649898\n",
            "Epoch 9400, Loss: 19.26107186649898\n",
            "Epoch 9500, Loss: 19.26107186649898\n",
            "Epoch 9600, Loss: 19.26107186649898\n",
            "Epoch 9700, Loss: 19.26107186649898\n",
            "Epoch 9800, Loss: 19.26107186649898\n",
            "Epoch 9900, Loss: 19.26107186649898\n",
            "Predictions:\n",
            "Звезды отражались -> душа\n",
            "Воздух стал -> воздух\n",
            "Она раньше шла -> её\n",
            "Дочка испугалась -> который\n",
            "Город погрузился -> вызвать\n",
            "Она не могла месяц -> говорить\n",
            "Электричество исчезло -> ещё\n",
            "Поиск видно -> пропасть\n"
          ]
        }
      ]
    }
  ]
}