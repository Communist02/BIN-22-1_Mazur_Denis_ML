{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhIEFApUNiiAE9fBY8Ka6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Communist02/BIN-22-1_Mazur_Denis_ML/blob/master/%D0%94%D0%97_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn nltk pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A6aLThJ5Nyh",
        "outputId": "92221980-6506-4126-eff5-0185309e6557"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8HZ6Klk4glE",
        "outputId": "44ccbca4-1615-4848-ddd9-bef50e99de23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import math\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "class Processing:\n",
        "    def tokenize(self, text: str) -> list[str]:\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def lematize(self, tokens: list[str]) -> list[str]:\n",
        "        morph3 = MorphAnalyzer()\n",
        "        lemmatized_words = [morph3.parse(\n",
        "            word)[0].normal_form for word in tokens]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def stemming(self, tokens: list[str]) -> list[str]:\n",
        "        stemmer = SnowballStemmer(\"english\")\n",
        "        lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def vectorize(self, tokens: list[str]) -> list[int]:\n",
        "        dict_vectors = {}\n",
        "        result = []\n",
        "        for word in tokens:\n",
        "            if word in dict_vectors.keys():\n",
        "                result.append(dict_vectors[word])\n",
        "            else:\n",
        "                dict_vectors[word] = len(dict_vectors)\n",
        "                result.append(dict_vectors[word])\n",
        "        return result\n",
        "\n",
        "    def vectorize_dict(self, tokens: list[str]) -> list[int]:\n",
        "        dict_vectors = {}\n",
        "        for word in tokens:\n",
        "            if word not in dict_vectors.keys():\n",
        "                dict_vectors[word] = len(dict_vectors)\n",
        "        return dict_vectors\n",
        "\n",
        "    def delete_stop_words(self, tokens: list[str]) -> list[int]:\n",
        "        stop_words = set(stopwords.words('english')).union(\n",
        "            ['.', ',', ':', '?', '!', '(', ')'])\n",
        "        return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    def bag_of_words(self, tokens: list[str]) -> dict[str]:\n",
        "        dict_words = {}\n",
        "        for word in tokens:\n",
        "            dict_words[word] = dict_words.setdefault(word, 0) + 1\n",
        "        return dict_words\n",
        "\n",
        "    def tf(self, tokens: list[str]) -> dict[str]:\n",
        "        dict_words = self.bag_of_words(tokens)\n",
        "        for word in dict_words:\n",
        "            dict_words[word] /= len(tokens)\n",
        "        return dict_words\n",
        "\n",
        "    def idf(self, texts: list[list[str]]) -> dict[str]:\n",
        "        dict_words = {}\n",
        "        big_text = []\n",
        "        for text in texts:\n",
        "            big_text += list(set(text))\n",
        "        for word in set(big_text):\n",
        "            dict_words[word] = math.log(len(texts) / big_text.count(word))\n",
        "        return dict_words\n",
        "\n",
        "    def tf_idf(self, texts: list[list[str]], indexText: int) -> dict[str]:\n",
        "        tf = self.tf(texts[indexText])\n",
        "        idf = self.idf(texts)\n",
        "        dict_words = {}\n",
        "        for word in tf:\n",
        "            dict_words[word] = tf[word] * idf[word]\n",
        "        return dict_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The AG News Corpus is a popular dataset commonly used for text classification tasks in Natural Language Processing (NLP). It consists of news articles collected from the AG's corpus of news articles on the web, categorized into four classes: World, Sports, Business, and Science/Technology. Each article is accompanied by a title and a short description, making it suitable for tasks like topic classification and sentiment analysis. With its diverse range of topics and well-labeled categories, the AG News Corpus serves as a valuable resource for training and evaluating machine learning models in various NLP applications.\n",
        "\n",
        "# Description:\n",
        "\n",
        "# Dataset: AG News Corpus\n",
        "# Source: AG's corpus of news articles on the web.\n",
        "# Content: News articles categorized into World, Sports, Business, and Science/Technology.\n",
        "# Labels: Four class labels representing different news categories.\n",
        "# Scope: Covers a broad range of current events and topics.\n",
        "# Size: Typically contains thousands of articles.\n",
        "# Language: Primarily in English."
      ],
      "metadata": {
        "id": "fCTWcs-14t0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "pc = Processing()\n",
        "\n",
        "y = df['Class Index']\n",
        "x = df.copy().drop('Class Index', axis=1)"
      ],
      "metadata": {
        "id": "tGd-IsgD41DA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "B4IBGwj054Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    text = row['Title']\n",
        "    tokens = pc.tokenize(text)  # Токенизация\n",
        "    tokens = pc.lematize(tokens)  # Лемматизация\n",
        "    tokens = pc.delete_stop_words(tokens)  # Удаление стоп слов\n",
        "\n",
        "    row['Title'] = ' '.join(tokens)\n",
        "\n",
        "    text = row['Description']\n",
        "    tokens = pc.tokenize(text)  # Токенизация\n",
        "    tokens = pc.lematize(tokens)  # Лемматизация\n",
        "    tokens = pc.delete_stop_words(tokens)  # Удаление стоп слов\n",
        "\n",
        "    row['Description'] = ' '.join(tokens)"
      ],
      "metadata": {
        "id": "7MA5sNxl5ITF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train, y_train, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "a1cJicbN5Kgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}